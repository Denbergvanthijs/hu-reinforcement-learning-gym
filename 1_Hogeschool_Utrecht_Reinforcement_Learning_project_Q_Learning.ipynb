{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Hogeschool-Utrecht_Reinforcement-Learning-project_Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denbergvanthijs/hu-reinforcement-learning-gym/blob/master/1_Hogeschool_Utrecht_Reinforcement_Learning_project_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eU2QJ--T4dd"
      },
      "source": [
        "# Reinforcement Learning project - Q-Learning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qdkwbpRUhzO"
      },
      "source": [
        "## Aim\n",
        "In this lab we are going to solve two simple [OpenAI Gym](https://gym.openai.com/) environments using [Q-Learning](https://en.wikipedia.org/wiki/Q-learning). Specifically, the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0/) and [MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0/) environments.\n",
        "\n",
        "We will try to create a table containing the expected reward for each combination of a *state* and *action*. We will use this table to choose the (hopefully) best action given the state the system is in.\n",
        "\n",
        "While this may not be the most advanced or complicated model there is, it is perfect for this task! Furthermore, it can be trained in a relatively short time!\n",
        "\n",
        "## Runtime and environment\n",
        "This [Jupyter Notebook](https://jupyterlab.readthedocs.io/en/latest/) was made to run on Google Colab. For this training, we recommend using the Google Colab environment.\n",
        "\n",
        "Please read the [instructions on Google Colab](https://medium.com/swlh/the-best-place-to-get-started-with-ai-google-colab-tutorial-for-beginners-715e64bb603b) to get started quickly. It behaves similar to Jupyter Notebook, Jupyter Hub and Jupyter Lab, so if you have any experience with those, you're good to go!\n",
        "\n",
        "Some notes on Google Colab:\n",
        "- **Processes in Google Colab won't run forever**. These may be terminated at any time when the platform is crowded, and *will definitely* terminate after 12 hours. To maintain persistency, you can attach the session to **Google Drive** and have your models persist themselves to the Google Drive periodically.\n",
        "- You can enable GPU or TPU support! You can find this option under *Runtime* -> *Change runtime type*.\n",
        "- After installing dependencies, you need to restart the runtime in order to actually use them.\n",
        "\n",
        "If you want to run the code on your own platform or system, you need to keep a few things in mind:\n",
        "- The dependencies you need to install may differ from the ones we installed here. The installed dependencies are suitable for Google Colab, Ubuntu, and Debian.\n",
        "- Since Google Colab isn't attached to a monitor, we render the output to a video file. On your own machine the built-in render method from OpenAI's Gym may suffice.\n",
        "- The default paths use Google Drive! Change these.\n",
        "\n",
        "## Info Support\n",
        "This assignment was developed by Info Support. Looking for a graduation project or job? Check out their website: https://carriere.infosupport.com/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7mfjQyuT_zV"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LwZ18PXsaL"
      },
      "source": [
        "Some dependencies need to be installed for the code to work. Furthermore, we will define some methods which allow us to show the OpenAI Gym renderings in this (headless) Google Colab environment.\n",
        "\n",
        "You only have to run these and don't need to change any of the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBdwK87YUI9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1831b2d-0663-484e-e8aa-5538dd99979c"
      },
      "source": [
        "# Install dependencies\n",
        "\"\"\"Note: if you are running this code on your own machine, you probably don't need all of these.\n",
        "   Start with 'pip install gym' and install more packages if you run into errors.\"\"\"\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg cmake > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install colabgymrender"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.2.0)\n",
            "Requirement already satisfied: colabgymrender in /usr/local/lib/python3.7/dist-packages (1.0.8)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (0.2.3.5)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (4.1.2.30)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from colabgymrender) (5.5.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (1.19.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy->colabgymrender) (2.4.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay->colabgymrender) (0.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (56.2.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->colabgymrender) (4.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy->colabgymrender) (7.1.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->colabgymrender) (0.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->colabgymrender) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->colabgymrender) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->colabgymrender) (0.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7IG_wxod9mW"
      },
      "source": [
        "# Imports for helper functions\n",
        "import base64\n",
        "import io\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from colabgymrender.recorder import Recorder\n",
        "from google.colab import drive\n",
        "from gym.wrappers import Monitor\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFDD54_Afgs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc291cbe-9da0-4763-a8a0-d131a7ee4dd9"
      },
      "source": [
        "# Mount your Google Drive. By doing so, you can store any output, models, videos, and images persistently.\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJeiCQFRgnSj"
      },
      "source": [
        "# Create a directory to store the data for this lab. Feel free to change this.\n",
        "data_path = Path('/content/gdrive/My Drive/Colab Notebooks/HU_RL/part1')\n",
        "data_path.mkdir(parents=True, exist_ok=True)\n",
        "video_path = data_path / 'video'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU2ZsnBEelqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89b63ae-8857-4641-c255-7d90ce822ff4"
      },
      "source": [
        "# Define helper functions to visually show what the models are doing.\n",
        "%matplotlib inline\n",
        "\n",
        "gym.logger.set_level(gym.logger.ERROR)\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "def show_video():\n",
        "    # Display the stored video file\n",
        "    # Credits: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "    mp4list = list(data_path.glob('video/*.mp4'))\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[-1]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print('Could not find video')\n",
        "\n",
        "\n",
        "def record_episode(idx):\n",
        "    # This determines which episodes to record.\n",
        "    # Since the video rendering in the OpenAI Gym is a bit buggy, we simply override it and decide\n",
        "    # whether or not to render inside of our training loop.\n",
        "    return True\n",
        "\n",
        "    \n",
        "def video_env(env):\n",
        "    # Wraps the environment to write its output to a video file\n",
        "    env = Monitor(env, video_path, video_callable=record_episode, force=True)\n",
        "    return env\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f1d1843ec50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShMPjUFXiwli"
      },
      "source": [
        "# Test the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3hAfAGhi4KK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648eafee-c681-4c2a-fc11-5358ab42e85a"
      },
      "source": [
        "print(\"\"\"We will use a basic OpenAI Gym examle: CartPole-v0.\n",
        "In this example, we will try to balance a pole on a cart.\n",
        "This is similar to kids (and.. grown-ups) trying to balance sticks on their hands.\n",
        "\n",
        "Check out the OpenAI Gym documentation to learn more: https://gym.openai.com/docs/\"\"\")\n",
        "\n",
        "# Create the desired environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Wrap the environment, to make sure we get to see a fancy video\n",
        "env = video_env(env)\n",
        "\n",
        "# # Before you can use a Gym environment, it needs to be reset.\n",
        "state = env.reset()\n",
        "\n",
        "# Perform random actions untill we drop the stick. Just as an example.\n",
        "done = False\n",
        "while not done:\n",
        "    # env.render()\n",
        "    # The action_space contains all possible actions we can take.\n",
        "    random_action = env.action_space.sample() \n",
        "\n",
        "    # After each action, we end up in a new state and receive a reward.\n",
        "    # When we drop the pole (more than 12 degrees), or balance it long enough (200 steps),\n",
        "    # or drive off the screen, done is set to True.\n",
        "    state, reward, done, info = env.step(random_action)\n",
        "\n",
        "# Show the results!\n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We will use a basic OpenAI Gym examle: CartPole-v0.\n",
            "In this example, we will try to balance a pole on a cart.\n",
            "This is similar to kids (and.. grown-ups) trying to balance sticks on their hands.\n",
            "\n",
            "Check out the OpenAI Gym documentation to learn more: https://gym.openai.com/docs/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-O3zaP6q4-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be31b6ef-6739-4842-ae75-33934007d8c1"
      },
      "source": [
        "# Neat, it did something (randomly)! \n",
        "\n",
        "# In order to train the system, we will try to predict the reward a certain actions yields given the state of the system.\n",
        "# But what is the state anyway?\n",
        "\n",
        "# In this environment, the state represents the cart's position and velocity, and the pole's angle and velocity.\n",
        "\n",
        "# Let's check out the current state\n",
        "print(f'Cart position: {state[0]} (range: [-4.8, 4.8])')\n",
        "print(f'Cart velocity: {state[1]} (range: [-inf, inf])')\n",
        "print(f'Pole angle: {state[2]} (range: [-0.418, 0.418])')\n",
        "print(f'Pole velocity: {state[3]} (range [-inf, inf])')\n",
        "\n",
        "# You can find out the minimum and maximum possible observation values using:\n",
        "print(f'Low observation space:', env.observation_space.low)\n",
        "print(f'High observation space:', env.observation_space.high)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cart position: -0.1653164095351859 (range: [-4.8, 4.8])\n",
            "Cart velocity: -1.1492470361240874 (range: [-inf, inf])\n",
            "Pole angle: 0.24870435931351403 (range: [-0.418, 0.418])\n",
            "Pole velocity: 2.03689216823342 (range [-inf, inf])\n",
            "Low observation space: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
            "High observation space: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhQlBt2hUAQ2"
      },
      "source": [
        "# Implement Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFsyJOo5GrZe"
      },
      "source": [
        "## Task\n",
        "Implement Q-Learning and find suitable parameters to reach a 200 reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvyHdRyRtdq"
      },
      "source": [
        "# Define parameters - Fill in the dots\n",
        "\n",
        "num_episodes = 20_000\n",
        "num_episodes_between_status_update = 5_000\n",
        "num_episodes_between_videos = num_episodes\n",
        "\n",
        "learning_rate = 0.1          # also known as: alpha\n",
        "discount = 0.95              # also known as: gamma\n",
        "epsilon = 0.99\n",
        "\n",
        "# Epsilon decay\n",
        "pass    # Optionally, add parameters for epsilon decay here\n",
        "\n",
        "# Discretization\n",
        "pass    # You can add parameters to discretize states here"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0S8rYCWT15s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce84cd5-ea87-4129-d067-4a83bbefe3d8"
      },
      "source": [
        "## Q-Table creation\n",
        "\n",
        "# As seen before, the state consists of 4 floating point values.\n",
        "# It makes sense to discretize these values (read: place them in buckets), to reduce the state space and therefore the Q-table size\n",
        "# state_shape = [5, 101, 203, 157]      # For instance: [4, 4, 6, 6], or [10] * 4, or [200, 200, 100, 100]\n",
        "# state_shape = [5, 5, 5, 5]      # For instance: [4, 4, 6, 6], or [10] * 4, or [200, 200, 100, 100]\n",
        "state_shape = [35, 35, 51, 51]      # For instance: [4, 4, 6, 6], or [10] * 4, or [200, 200, 100, 100]\n",
        "\n",
        "# Define the initial Q table as a random uniform distribution\n",
        "q_table = np.random.uniform(low=-1, high=0, size=(state_shape + [env.action_space.n]))\n",
        "\n",
        "space_space = np.vstack((env.observation_space.low, env.observation_space.high, state_shape)).T\n",
        "# space_space[1,:2] = np.clip(space_space[1,:2], -2, 2)\n",
        "# space_space[3,:2] = np.clip(space_space[3,:2], -2, 2)\n",
        "space_space[:,:2] = np.clip(space_space[:,:2], -10, 10)\n",
        "space_space_linspace = [np.linspace(low, high, num=int(num)) for low, high, num in space_space]\n",
        "print(len(space_space_linspace), [len(space_linspace) for space_linspace in space_space_linspace])\n",
        "\n",
        "print('Q table shape:', q_table.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 [35, 35, 51, 51]\n",
            "Q table shape: (35, 35, 51, 51, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Cp8cNtUcDf"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SMMA_KceHrx"
      },
      "source": [
        "def discretize_state(state, bins=space_space_linspace):\n",
        "    # A Q-table cannot practically handle infinite states, so limit the state space by\n",
        "    # discretizing the state into buckets.\n",
        "    discrete_state = np.array([np.digitize(s, bun) for s, bun in zip(state, bins)])\n",
        "    # print(state)\n",
        "    # print(discrete_state)\n",
        "    return tuple(discrete_state.astype(np.int))\n",
        "\n",
        "def take_action(discrete_state, epsilon, q_table=q_table,\n",
        "                rng=np.random.default_rng(), env=env):\n",
        "    # Take an action to either explore or exploit.\n",
        "    if rng.random() > epsilon:\n",
        "      # Exploit\n",
        "      action = np.argmax(q_table[discrete_state])\n",
        "    else:\n",
        "      # Explore\n",
        "      action = rng.integers(env.action_space.n)\n",
        "    return action\n",
        "\n",
        "def estimated_max_for_next_state(discrete_state, q_table=q_table):\n",
        "    # What's the best expected Q-value for the next state?\n",
        "    estimated_max = np.max(q_table[discrete_state])\n",
        "    return estimated_max\n",
        "\n",
        "def new_q_value(discrete_state, action, max_future_q, reward, q_table=q_table,\n",
        "                learning_rate=learning_rate, discount=discount):\n",
        "    # Calculate the new Q-value\n",
        "    current_q = q_table[discrete_state + (action,)]\n",
        "    new_q = current_q + learning_rate * (reward + discount * max_future_q - current_q)\n",
        "    return new_q\n",
        "\n",
        "def decayed_epsilon(episode, epsilon_max: float = 0.99, epsilon_min: float = 0.01, steps: int = 10_000):\n",
        "    if episode > steps:\n",
        "        return epsilon_min\n",
        "    else:\n",
        "        epsilon_per_step = (epsilon_max - epsilon_min) / steps\n",
        "        return epsilon_max - epsilon_per_step * episode"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMUc3G164cA7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "c9d21c0d-bedc-4ae7-ccba-02a8c9abac3b"
      },
      "source": [
        "# Time to train the system\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset() # Don't forget to reset the environment between episodes\n",
        "    current_state_disc = discretize_state(state)\n",
        "\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        # env.render()\n",
        "        if (episode + 1) % num_episodes_between_videos == 0:\n",
        "            env.render()\n",
        "\n",
        "        # Take an action by exploration or exploitation\n",
        "        action = take_action(current_state_disc, epsilon)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_disc = discretize_state(new_state)\n",
        "\n",
        "        # Calculate the total reward\n",
        "        reward_sum += reward\n",
        "\n",
        "        if not done:\n",
        "            # Retrieve the maximum estimated value for the next state\n",
        "            max_future_q = estimated_max_for_next_state(new_state_disc)\n",
        "\n",
        "            # Calculate the new value (note: Bellman equation)\n",
        "            # Added reward as argument for Bellman equation (Casper & Thijs)\n",
        "            new_q = new_q_value(current_state_disc, action, max_future_q, reward)\n",
        "            q_table[current_state_disc + (action,)] = new_q\n",
        "        else:\n",
        "            # Render the video\n",
        "            if (episode + 1) % num_episodes_between_status_update == 0:\n",
        "                env.render()\n",
        "                print(epsilon)\n",
        "                print(f'Total reward at episode {episode + 1}: {reward_sum}')\n",
        "\n",
        "        # Prepare for the next loop\n",
        "        current_state_disc = new_state_disc\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = decayed_epsilon(episode)\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-36f6ce5ed378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mnew_state_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscretize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Calculate the total reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d339c728d3b8>\u001b[0m in \u001b[0;36mdiscretize_state\u001b[0;34m(state, bins)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# A Q-table cannot practically handle infinite states, so limit the state space by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# discretizing the state into buckets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdiscrete_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# print(discrete_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-d339c728d3b8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# A Q-table cannot practically handle infinite states, so limit the state space by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# discretizing the state into buckets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdiscrete_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# print(discrete_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(x, bins, right)\u001b[0m\n\u001b[1;32m   4771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4772\u001b[0m     \u001b[0;31m# here for compatibility, searchsorted below is happy to take this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4773\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplexfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4774\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x may not be complex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \"\"\"\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RbC9GKag15q"
      },
      "source": [
        "# MountainCar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0oyL2bxg_1j"
      },
      "source": [
        "Now apply the things you've learned to the MountainCar problem. Please note that the observable space differs from the previous problem. Thus, before you start training, you need to learn more about the/this new environment.\n",
        "\n",
        "Here is some code to help you get started..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw7Vn_PehiDW"
      },
      "source": [
        "# Create the desired environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Wrap the environment, to make sure we get to see a fancy video\n",
        "# env = video_env(env)\n",
        "\n",
        "# Before you can use a Gym environment, it needs to be reset.\n",
        "state = env.reset()\n",
        "\n",
        "learning_rate = 0.1\n",
        "discount = 0.99\n",
        "num_episodes = 20_000\n",
        "num_episodes_between_videos = 20_000\n",
        "num_episodes_between_status_update = 2_500\n",
        "epsilon = 0.99\n",
        "epsilon_max = 0.99\n",
        "epsilon_min = 0.01\n",
        "steps = 10_000"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmtG6Fwte6Ur",
        "outputId": "459530f4-1716-40bc-e9f2-0ab96dadb07d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's check out the current state\n",
        "print(f'Car Position: {state[0]}')\n",
        "print(f'Car Velocity: {state[1]}')\n",
        "\n",
        "# You can find out the minimum and maximum possible observation values using:\n",
        "print(f'Low observation space:', env.observation_space.low)\n",
        "print(f'High observation space:', env.observation_space.high)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Car Position: -0.406683953331528\n",
            "Car Velocity: 0.0\n",
            "Low observation space: [-1.2  -0.07]\n",
            "High observation space: [0.6  0.07]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_1drEnZfPXy",
        "outputId": "6e4f8286-0d25-4001-fad1-ce3b6dc76ab0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "state_shape = [51, 51]      # For instance: [4, 4, 6, 6], or [10] * 4, or [200, 200, 100, 100]\n",
        "\n",
        "# Define the initial Q table as a random uniform distribution\n",
        "q_table = np.random.uniform(low=-1, high=0, size=(state_shape + [env.action_space.n]))\n",
        "\n",
        "space_space = np.vstack((env.observation_space.low, env.observation_space.high, state_shape)).T\n",
        "\n",
        "space_space[:,:2] = np.clip(space_space[:,:2], -1.3, 0.7)\n",
        "space_space_linspace = [np.linspace(low, high, num=int(num)) for low, high, num in space_space]\n",
        "print(len(space_space_linspace), [len(space_linspace) for space_linspace in space_space_linspace])\n",
        "\n",
        "print('Q table shape:', q_table.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 [51, 51]\n",
            "Q table shape: (51, 51, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzVnM10g4Oc",
        "outputId": "5ec0d4ad-56cc-47e3-bd0d-b3510d59dfe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Time to train the system\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset() # Don't forget to reset the environment between episodes\n",
        "    current_state_disc = discretize_state(state, bins=space_space_linspace)\n",
        "\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        # env.render()\n",
        "        if (episode + 1) % num_episodes_between_videos == 0:\n",
        "            pass\n",
        "            # env.render()\n",
        "\n",
        "        # Take an action by exploration or exploitation\n",
        "        action = take_action(current_state_disc, epsilon, q_table=q_table, env=env)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        new_state_disc = discretize_state(new_state, bins=space_space_linspace)\n",
        "\n",
        "        # Calculate the total reward\n",
        "        reward_sum += reward\n",
        "\n",
        "        if not done:\n",
        "            # Retrieve the maximum estimated value for the next state\n",
        "            max_future_q = estimated_max_for_next_state(new_state_disc, q_table=q_table)\n",
        "\n",
        "            # Calculate the new value (note: Bellman equation)\n",
        "            # Added reward as argument for Bellman equation (Casper & Thijs)\n",
        "            new_q = new_q_value(current_state_disc, action, max_future_q, reward,\n",
        "                                q_table=q_table, learning_rate=learning_rate,\n",
        "                                discount=discount)\n",
        "            q_table[current_state_disc + (action,)] = new_q\n",
        "        else:\n",
        "            # Render the video\n",
        "            if (episode + 1) % num_episodes_between_status_update == 0:\n",
        "                # env.render()\n",
        "                print(f'Total reward at episode {episode + 1}: {reward_sum}; epsilon={epsilon}')\n",
        "\n",
        "        # Prepare for the next loop\n",
        "        current_state_disc = new_state_disc\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = decayed_epsilon(episode, epsilon_max=epsilon_max, epsilon_min=epsilon_min, steps=steps)\n",
        "\n",
        "# Show the results!\n",
        "env.close()\n",
        "# show_video()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward at episode 2500: -200.0; epsilon=0.745196\n",
            "Total reward at episode 5000: -200.0; epsilon=0.5001960000000001\n",
            "Total reward at episode 7500: -200.0; epsilon=0.255196\n",
            "Total reward at episode 10000: -191.0; epsilon=0.010195999999999983\n",
            "Total reward at episode 12500: -167.0; epsilon=0.01\n",
            "Total reward at episode 15000: -185.0; epsilon=0.01\n",
            "Total reward at episode 17500: -120.0; epsilon=0.01\n",
            "Total reward at episode 20000: -154.0; epsilon=0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpdd_0p8mBgO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}